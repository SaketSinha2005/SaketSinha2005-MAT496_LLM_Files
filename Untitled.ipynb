{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a176ae-56f4-4d53-8363-8f2a7d012d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7d5cb8-2dcd-4e5f-b05f-21d7b9407b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    raise ValueError(\"ROQ_API_KEY not found.\")\n",
    "\n",
    "model = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b248949-d4ba-4b88-ad91-9925e0d3a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    SystemMessage(\"Dont add any other text other than asked thing\"),\n",
    "    HumanMessage(\"Give me a tricky iq testing question\"),\n",
    "    ]\n",
    "response=model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886715db-02a6-421c-ad48-42d99aedd6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are three switches, but they are not labelled. Each switch corresponds to one of three light bulbs in a room. Each bulb is off at the start. You can turn the lights on and off as many times as you want, but you can only enter the room to observe the bulbs one time. How can you figure out which switch corresponds to which light bulb?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333e756-b2b7-4f8a-a1c4-22b96bed05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
