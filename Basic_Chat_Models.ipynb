{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db267d51-2a39-46c6-ae9b-b9189e242915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320de5df-7d6c-45f2-9032-a571814ce99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbedc5a-45b5-482a-acb4-35f46b6e531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    raise ValueError(\"ROQ_API_KEY not found.\")\n",
    "\n",
    "+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "501bebcc-de18-402d-bd89-415aa70727f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Saket. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "response = model.invoke([HumanMessage(content=\"Hi! I'm Saket\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21623281-0a35-4d74-b104-5c29296c494c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about your name. I'm a large language model, I don't have the ability to retain information about individual users or their personal details. Each time you interact with me, it's a new conversation and I don't have any prior knowledge.\\n\\nIf you'd like to share your name with me, I'd be happy to chat with you and use it in our conversation. Alternatively, we can stick to a neutral name or not use a name at all if you prefer.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 40, 'total_tokens': 142, 'completion_time': 0.466424405, 'prompt_time': 0.494320231, 'queue_time': 0.069610197, 'total_time': 0.960744636}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_510c177af0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8f608462-7aad-4766-bb26-65bd27b51277-0', usage_metadata={'input_tokens': 40, 'output_tokens': 102, 'total_tokens': 142})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f79e464f-b4db-4357-8447-0a9726b7f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Saket.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 67, 'total_tokens': 74, 'completion_time': 0.005387133, 'prompt_time': 0.011751836, 'queue_time': 0.045848594, 'total_time': 0.017138969}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_46fc01befd', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--70fa4908-7a4a-43e2-8f4d-4fca5eab1f42-0', usage_metadata={'input_tokens': 67, 'output_tokens': 7, 'total_tokens': 74})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Saket\"),\n",
    "        AIMessage(content=\"Hello Saket! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4257f892-9915-4909-9202-10038edf6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| color| of| the| sky| can| vary| depending| on| the| time| of| day| and| atmospheric| conditions|.| \n",
      "\n",
      "|-| During| the| daytime|,| when| the| sun| is| overhead|,| the| sky| typically| appears| blue|.| This| is| because| shorter| wavelengths| of| light|,| such| as| blue| and| violet|,| are| scattered| more| than| longer| wavelengths|,| such| as| red| and| orange|,| by| the| tiny| molecules| of| gases| in| the| atmosphere|.| This| phenomenon| is| known| as| Ray|leigh| scattering|.\n",
      "\n",
      "|-| During| sunrise| and| sunset|,| the| sky| can| appear| red|,| orange|,| or| pink| due| to| a| different| set| of| conditions|.| In| these| instances|,| the| sun|'s| light| has| to| travel| through| more| of| the| Earth|'s| atmosphere| to| reach| our| eyes|,| which| sc|atters| the| shorter| wavelengths| of| light|,| allowing| longer| wavelengths| like| red| and| orange| to| dominate| the| color| we| see|.\n",
      "\n",
      "|-| At| night|,| when| the| sun| is| not| visible|,| the| sky| can| appear| dark|,| with| stars| and| other| celestial| objects| visible|.\n",
      "\n",
      "|-| In| the| presence| of| pollution|,| dust|,| or| other| particles| in| the| atmosphere|,| the| sky| can| appear| h|azy| or| gray|.\n",
      "\n",
      "|-| During| severe| weather| conditions|,| such| as| heavy| rain| or| thunder|storms|,| the| sky| can| appear| dark| and| fore|b|oding|.\n",
      "\n",
      "|So|,| the| color| of| the| sky| can| vary| depending| on| a| range| of| factors|,| but| in| clear|,| sunny| conditions|,| it| typically| appears| blue|.||"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in model.stream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4abc6269-b76e-40b0-bc4c-aa3f67d93284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao a tutti', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 44, 'total_tokens': 49, 'completion_time': 0.004442612, 'prompt_time': 0.350265023, 'queue_time': 0.046143496, 'total_time': 0.354707635}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_46fc01befd', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a5acf9ee-7fd9-403d-b816-529467619221-0', usage_metadata={'input_tokens': 44, 'output_tokens': 5, 'total_tokens': 49})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"Hello Everyone\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bb5ddda-284b-4297-b4e5-559a7e5edce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dec887f-b364-4d48-b6f4-6abe96280366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi everyone', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi everyone\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36fba61d-ad0c-4158-90c6-bd64bc284d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi everyone', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b80544f9-4d8d-4427-af90-d3686fc3cd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao a tutti\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391eebc-77d3-4a08-800b-063da5ae8b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
